# -*- coding: utf-8 -*-
"""Correcting_spellings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TgWkhjR5dTFIB7at03KFdPKO2pGadARz
"""

import re
from collections import Counter
import nltk
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
import pandas as pd

df = pd.read_csv('papers.csv')

nltk.download('stopwords')
nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
##Creating a list of custom stopwords
new_words = ["fig","figure","image","sample","using",
             "show", "result", "large",
             "also", "one", "two", "three",
             "four", "five", "seven","eight","nine"]
stop_words = list(stop_words.union(new_words))

def pre_process(text):

    # lowercase
    text=text.lower()

    #remove tags
    text=re.sub("&lt;/?.*?&gt;"," &lt;&gt; ",text)

    # remove special characters and digits
    text=re.sub("(\\d|\\W)+"," ",text)

    ##Convert to list from string
    text = text.split()

    # remove stopwords
    text = [word for word in text if word not in stop_words]

    # remove words less than three letters
    text = [word for word in text if len(word) >= 3]

    # lemmatize
    lmtzr = WordNetLemmatizer()
    text = [lmtzr.lemmatize(word) for word in text]

    return ' '.join(text)

docs = df['paper_text'].iloc[:3000].apply(lambda x:pre_process(x))

docs.shape

sentences = docs.tolist()
len(sentences)

sentences[:2]

text_data = ' '.join(sentences)
text_data[:1000]

def misc(file_name):
    words = []
    file_name = process_tweet(file_name)
    words = re.findall(r'\w+', file_name)
    return words

def process_tweet(tweet):
      tweet = re.sub(r'\$\w*', '', tweet)
      tweet = re.sub(r'^RT[\s]+', '', tweet)
      tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)
      tweet = re.sub(r'#', '', tweet)

      return tweet

words = misc(text_data)
vocab = set(words)
print(f"There are {len(vocab)} unique words in the vocabulary.")

# words = re.findall(r'\w+', text_data)
# print(len(words))
# vocab = set(words)
# print(len(vocab))

def get_count(word_l):
    """
    Input:
        word_l: a set of words representing the corpus.
    Output:
        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.
    """
    word_count_dict = {}
    word_count_dict = Counter(word_l)

    return word_count_dict

word_count_dict = get_count(words)

word_count_dict['infinite']

def get_probs(word_count_dict):
    """
    Input:
        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.
    Output:
        probs: A dictionary where keys are the words and the values are the probability that a word will occur.
    """
    probs = {}  # return this variable correctly
    m = sum(word_count_dict.values())
    for key in word_count_dict.keys():
        probs[key] = word_count_dict.get(key, 0) / m

    return probs

probs = get_probs(word_count_dict)

# Part 2: String Manipulation

# delete_letter()
def delete_letter(word, verbose=False):
    delete_l = []
    split_l = []

    split_l = [(word[:i], word[i:]) for i in range(len(word))]
    delete_l = [L + R[1:] for L, R in split_l if R]

    if verbose:
        print(f"input word {word}, \nsplit_l = {split_l}, \ndelete_l = {delete_l}")# printing implicitly.

    return delete_l

# checking the function
print(delete_letter(word="cans", verbose=True))

# switch_letter()
def switch_letter(word, verbose=False):
    def swap(c, i, j):
        c = list(c)
        c[i], c[j] = c[j], c[i]
        return ''.join(c)

    switch_l = []
    split_l = []
    split_l = [(word[:i], word[i:]) for i in range(len(word))]
    switch_l = [a + b[1] + b[0] + b[2:] for a, b in split_l if len(b) >= 2]

    if verbose:
        print(f"Input word = {word} \nsplit_l = {split_l} \nswitch_l = {switch_l}")

    return switch_l

print(switch_letter(word="eta", verbose=True))

# replace_letter()
def replace_letter(word, verbose=False):
    letters = 'abcdefghijklmnopqrstuvwxyz'
    replace_l = []
    split_l = []

    split_l = [(word[:i], word[i:]) for i in range(len(word))]
    replace_l = [a + l + (b[1:] if len(b) > 1 else '') for a, b in split_l if b for l in letters]
    replace_set = set(replace_l)
    replace_set.remove(word)
    # turn the set back into a list and sort it, for easier viewing
    replace_l = sorted(list(replace_set))

    if verbose:
        print(f"Input word = {word} \nsplit_l = {split_l} \nreplace_l {replace_l}")

    return replace_l

print(replace_letter(word='can', verbose=True))

#  insert_letter()
def insert_letter(word, verbose=False):
    letters = 'abcdefghijklmnopqrstuvwxyz'
    insert_l = []
    split_l = []
    split_l = [(word[:i], word[i:]) for i in range(len(word))]
    insert_l = [ a + l + b for a, b in split_l for l in letters]

    if verbose:
        print(f"Input word {word} \nsplit_l = {split_l} \ninsert_l = {insert_l}")

    return insert_l

print(insert_letter(word='at', verbose=False))

# Combining the edits:
# Now that you have implemented the string manipulations, you will create two functions that,
#  given a string, will return all the possible single and double edits on that string. These will
#  be edit_one_letter() and edit_two_letters().

#  Edit one letter
def edit_one_letter(word, allow_switches=True):

    edit_one_set = set()
    edit_one_set.update(delete_letter(word))
    if allow_switches:
        edit_one_set.update(switch_letter(word))
    edit_one_set.update(replace_letter(word))
    edit_one_set.update(insert_letter(word))

    return edit_one_set

# Edit two letters
def edit_two_letters(word, allow_switches=True):

    edit_two_set = set()
    edit_one = edit_one_letter(word, allow_switches=allow_switches)
    for w in edit_one:
        if w:
            edit_two = edit_one_letter(w, allow_switches=allow_switches)
            edit_two_set.update(edit_two)

    return edit_two_set

# proposed
edit_two_letters('proposal')

# suggest spelling suggestions
def get_corrections(word, probs, vocab, verbose=False):
    """
    Input:
        word: a user entered string to check for suggestions
        probs: a dictionary that maps each word to its probability in the corpus
        vocab: a set containing all the vocabulary
        n: number of possible word corrections you want returned in the dictionary
    Output:
        n_best: a list of tuples with the most probable n corrected words and their probabilities.
    """

    suggestions = []
    n_best = []
    #suggestions = list((word in vocab) or edit_one_letter(word).intersection(vocab) or
    #                   edit_two_letters(word).intersection(vocab))
    suggestions = list(edit_two_letters(word).intersection(vocab))
    # suggestions = list(edit_two_letters(word, False).intersection(vocab))
    n_best = [[s, probs.get(s, -1)] for s in list(reversed(suggestions))]

    if verbose:
        print("suggestions = ", suggestions)

    return n_best

# Testing
my_word = 'proposal'
tmp_corrections = get_corrections(my_word, probs, vocab, verbose=False)
for i, word_prob in enumerate(tmp_corrections):
    print(f"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}")

# Saving the dictionary and vocabulary:

import pickle

pickle.dump(probs, open('word-probability-spellings.pkl', 'wb'))
pickle.dump(vocab, open('vocab-spellings.pkl', 'wb'))